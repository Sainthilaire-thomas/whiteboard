// app/api/tts/route.ts - VERSION CORRIG√âE

import { NextRequest, NextResponse } from "next/server";
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// ‚úÖ TONS SP√âCIALIS√âS POUR LE CONSEIL
const toneInstructions = {
  professionnel:
    "Parlez d'un ton professionnel et courtois, adapt√© au conseil client√®le.",
  chaleureux:
    "Parlez d'un ton chaleureux et bienveillant, comme un ami qui aide.",
  enthousiaste:
    "Parlez avec enthousiasme et dynamisme, montrez de l'√©nergie positive.",
  calme: "Parlez d'un ton calme et rassurant, avec une voix apaisante.",
  confiant: "Parlez avec confiance et assurance, transmettez de la certitude.",
  explication:
    "Adoptez un ton p√©dagogique et structur√© pour expliquer clairement.",
  empathique:
    "Parlez avec compr√©hension et bienveillance, montrez de l'empathie.",
  resolution_probleme:
    "Utilisez un ton orient√© solution, dynamique et rassurant.",
  instructions:
    "Adoptez un ton clair et directif pour donner des instructions.",
  urgence_controlee:
    "Transmettez l'importance tout en restant calme et ma√Ætris√©.",
};

// ‚úÖ ANALYSE CONTEXTUELLE SIMPLIFI√âE
const analyzeTextContext = (text: string) => {
  const patterns = {
    explication: /(donc|ainsi|c'est-√†-dire|en effet|par cons√©quent)/i,
    empathique: /(je comprends|je vois|effectivement|d√©sol√©)/i,
    resolution_probleme: /(solution|r√©soudre|probl√®me|nous allons)/i,
    instructions: /(vous devez|il faut|suivez|proc√©dez|√©tape)/i,
    urgence_controlee: /(urgent|rapidement|important|attention)/i,
  };

  for (const [context, pattern] of Object.entries(patterns)) {
    if (pattern.test(text)) {
      return context;
    }
  }
  return "professionnel";
};

// ‚úÖ PREPROCESSING SIMPLIFI√â
const enhanceTextForProsodie = (text: string, detectedContext: string) => {
  let enhancedText = text.replace(/\s+/g, " ").trim();

  switch (detectedContext) {
    case "explication":
      enhancedText = enhancedText.replace(/(donc|ainsi)/gi, "... $1 ...");
      break;
    case "empathique":
      enhancedText = enhancedText.replace(
        /(je comprends|effectivement)/gi,
        "$1..."
      );
      break;
  }

  return enhancedText;
};

export async function POST(request: NextRequest) {
  try {
    const {
      text,
      voice = "alloy",
      speed = 1,
      model = "tts-1",
      tone,
      textEnhancement = "aucun",
      autoDetectContext = false,
    } = await request.json();

    console.log("üéôÔ∏è TTS Request:", {
      model,
      tone,
      textEnhancement,
      autoDetectContext,
    });

    let processedText = text;
    let effectiveTone = tone || "professionnel";

    // ‚úÖ ENHANCEMENT AUTOMATIQUE DU CONTEXTE
    if (autoDetectContext || textEnhancement === "contextuel") {
      const detectedContext = analyzeTextContext(text);
      console.log(`üé≠ Contexte d√©tect√©: ${detectedContext}`);

      if (!tone) {
        effectiveTone = detectedContext;
      }

      if (textEnhancement === "contextuel") {
        processedText = enhanceTextForProsodie(text, detectedContext);
        console.log(`üìù Texte enrichi: ${processedText}`);
      }
    }

    // ‚úÖ CORRECTION: Utiliser le bon nom de mod√®le
    if (model === "gpt-4o-mini-tts" || model === "gpt-4o-audio") {
      // ‚úÖ V√©rifier si le mod√®le GPT-4o-audio est disponible
      try {
        const selectedInstruction =
          toneInstructions[effectiveTone] || toneInstructions["professionnel"];

        const completion = await openai.chat.completions.create({
          model: "gpt-4o-audio-preview", // ‚úÖ MOD√àLE CORRECT
          modalities: ["text", "audio"],
          audio: { voice: voice, format: "mp3" },
          messages: [
            {
              role: "system",
              content: `${selectedInstruction}

Directives pour la prosodie:
- Respectez la ponctuation pour les pauses naturelles
- Modulez votre intonation selon le sens des phrases
- Gardez un ton professionnel adapt√© au conseil client√®le`,
            },
            {
              role: "user",
              content: processedText,
            },
          ],
        });

        const audioData = completion.choices[0].message.audio?.data;
        if (!audioData) {
          throw new Error("Pas de donn√©es audio g√©n√©r√©es");
        }

        const buffer = Buffer.from(audioData, "base64");
        return new NextResponse(buffer, {
          headers: {
            "Content-Type": "audio/mpeg",
            "Cache-Control": "no-cache",
            "X-Enhanced-Context": effectiveTone,
          },
        });
      } catch (audioError) {
        console.warn(
          "‚ùå GPT-4o-audio non disponible, fallback vers TTS standard:",
          audioError
        );
        // Fallback vers TTS standard
      }
    }

    // ‚úÖ API TTS STANDARD (toujours disponible)
    console.log("üîÑ Utilisation TTS standard");
    const mp3 = await openai.audio.speech.create({
      model:
        model === "gpt-4o-mini-tts" || model === "gpt-4o-audio"
          ? "tts-1"
          : model,
      voice: voice,
      input: processedText,
      speed: speed,
    });

    const buffer = Buffer.from(await mp3.arrayBuffer());
    return new NextResponse(buffer, {
      headers: {
        "Content-Type": "audio/mpeg",
        "Cache-Control": "no-cache",
        "X-Model-Used": model === "gpt-4o-mini-tts" ? "tts-1-fallback" : model,
      },
    });
  } catch (error: any) {
    console.error("‚ùå Erreur TTS compl√®te:", {
      message: error.message,
      status: error.status,
      code: error.code,
    });

    return NextResponse.json(
      {
        error: `Erreur TTS: ${error.message}`,
        details: error.code || "Unknown error",
      },
      { status: 500 }
    );
  }
}
